{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! It looks like you're testing things out. How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "def extract_answer(text):\n",
    "    pattern = r'<answer>(.*?)</answer>'\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def split_think_text(text):\n",
    "    # Find content inside <think> tags\n",
    "    think_pattern = r'<think>(.*?)</think>'\n",
    "    think_match = re.search(think_pattern, text, re.DOTALL)\n",
    "    \n",
    "    think_content = think_match.group(1).strip() if think_match else \"\"\n",
    "    \n",
    "    # Find content after </think>\n",
    "    after_pattern = r'</think>(.*)'\n",
    "    after_match = re.search(after_pattern, text, re.DOTALL)\n",
    "    \n",
    "    after_content = after_match.group(1).strip() if after_match else \"\"\n",
    "    \n",
    "    return think_content, after_content\n",
    "\n",
    "MODELS = {'DeepSeek-R1':'deepseek-ai/DeepSeek-R1',\n",
    "         'DeekSeek-R1-Zero':'deepseek-ai/DeepSeek-R1-Zero'}\n",
    "\n",
    "def hyperbolic_query(query, model=\"DeepSeek-R1\", temperature = 0.6):\n",
    "\n",
    "    url = \"https://api.hyperbolic.xyz/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + os.getenv(\"HYPERBOLIC_API\")\n",
    "    }\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ],\n",
    "        \"model\": MODELS[model],\n",
    "        \"max_tokens\": 10000, # need to set it quite high because of reasoning\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": 1.0\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()['choices'][0]['message']['content']\n",
    "\n",
    "print(hyperbolic_query(\"Hello Test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading story data \n",
    "scenarios = pd.read_csv(\"ToM Story.csv\")\n",
    "\n",
    "# Update character's names\n",
    "charA_name = \"John\"\n",
    "charB_name = \"Sam\"\n",
    "charC_name = \"Mike\"\n",
    "scenarios['Story'] = scenarios['Story'].str.replace('{charA}',charA_name)\n",
    "scenarios['Story'] = scenarios['Story'].str.replace('{charB}',charB_name)\n",
    "scenarios['Story'] = scenarios['Story'].str.replace('{charC}',charC_name)\n",
    "\n",
    "# Creating Questions \n",
    "questions = [ # Knowledge\n",
    "              # 'Is the {story_name} at the {L3}?',\n",
    "              # L0\n",
    "              #'Does {charB} think the {story_name} is at the {L3}?',\n",
    "              'Does {charA} think the {story_name} is heading to  the {L3}?',\n",
    "              # L1\n",
    "              'Does {charA} think that {charB} thinks that the {story_name} is heading to the {L3}?',\n",
    "              # L2\n",
    "              'Does {charA} think that {charB} thinks that {charA} thinks that the {story_name} is heading to the {L3}?',\n",
    "              #'Pretend to be {charA}. Do you think that {charB} thinks that you think that the {story_name} is at the {L3}?',\n",
    "              # charC questions (depends on the scenario)\n",
    "              #'Does {charC} think the {story_name} is at the {L3}?',\n",
    "              'Does {charC} think that {charB} is heading to the {L3}?',\n",
    "              'Does {charC} plan to go to the {L3} to see {charB}?',\n",
    "              'Where will {charC} go to see {charB}?'\n",
    "              ]\n",
    "answers = ['Yes','Yes','No','No','','','']\n",
    "\n",
    "# Generate prompts combining story and question \n",
    "dat = []\n",
    "for idx in scenarios.index:\n",
    "    for i in range(len(questions)):\n",
    "        instruction = \"Read the story below and answer a question.\\n\\n\"\n",
    "\n",
    "        question = questions[i].replace('{charA}',charA_name).replace('{charB}',charB_name).replace('{charC}',charC_name)\n",
    "        question = question.replace('{story_name}',scenarios.loc[idx]['story_name'])\n",
    "        question = question.replace('{L3}',scenarios.loc[idx]['L3'])\n",
    "        formatting = \" Put the final answer (e.g., yes/no/no answer or a location) in <answer> tags\"\n",
    "        prompt = instruction + scenarios.loc[idx]['Story'] + '\\n\\nQuestion: ' + question + formatting\n",
    "        temp = {'prompt':prompt, \n",
    "                'version':scenarios.loc[idx]['Version'],\n",
    "                'theme':scenarios.loc[idx]['Theme'],\n",
    "                'ending':scenarios.loc[idx]['Ending'],\n",
    "                'question':i+1}\n",
    "        dat.append(temp)\n",
    "dat = pd.DataFrame(dat)\n",
    "## Save to file \n",
    "dat.to_csv('ToM Story with question.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Responses\n",
    "\n",
    "- Repeat 10 times per questions \n",
    "- Store thinking and answer (only the one in <answer>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('ToM Story with question.csv')\n",
    "# Filter out due to time constraint\n",
    "dat = dat[(dat['version']=='Short') & dat['ending'].isin([\"charC's Question\",\n",
    "                                                          \"charA's response (complete) \"])]\n",
    "dat.to_csv('Tom Story with question (lite).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_results = pd.read_csv('ToM Story with question (lite) - R1.csv')\n",
    "r1_results['Answer 1'] = ''\n",
    "r1_results['Answer 2'] = ''\n",
    "r1_results['Answer 3'] = ''\n",
    "r1_results['Answer 4'] = ''\n",
    "r1_results['Answer 5'] = ''\n",
    "r1_results.to_csv('ToM Story with question (lite) - R1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_responses_cont(dat, model, file_name, n=5):\n",
    "    for idx in dat.index:\n",
    "        for i in range(n):\n",
    "            col_name = f'Answer {i+1}'\n",
    "            if pd.isna(dat.loc[idx][col_name]):\n",
    "                response = hyperbolic_query(dat.loc[idx]['prompt'], model)\n",
    "                dat.loc[idx, col_name] = response\n",
    "                dat.to_csv(file_name,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ToM Story with question (lite) - R1.csv'\n",
    "r1_results = pd.read_csv(file_name, encoding='latin1')\n",
    "gen_responses_cont(r1_results, 'DeepSeek-R1', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Not used below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def gen_responses(prompts, model, i, echo=False):\n",
    "    temp_df = prompts.copy()\n",
    "    temp_df['thinking'] = ''\n",
    "    temp_df['full_answer'] = ''\n",
    "    temp_df['final_answer'] = ''\n",
    "    temp_df['n'] = i\n",
    "    for idx in temp_df.index:\n",
    "        if echo:\n",
    "            print(temp_df.loc[idx]['prompt'])\n",
    "        query = temp_df.loc[idx]['prompt'] \n",
    "        response = hyperbolic_query(query, model)\n",
    "        outputs = split_think_text(response)\n",
    "        temp_df.loc[idx]['thinking'] = outputs[0]\n",
    "        temp_df.loc[idx]['full_answer'] = outputs[1]\n",
    "        temp_df.loc[idx]['full_answer'] = extract_answer(outputs[1])\n",
    "        if echo:\n",
    "            print(response)\n",
    "    temp_df.to_csv(f'result_{model}_{i}.csv')  \n",
    "    return temp_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R1\n",
    "n = 5\n",
    "model = 'DeepSeek-R1'\n",
    "for i in range(n):\n",
    "    print(i)\n",
    "    temp_df = gen_responses(dat, model, i)\n",
    "    print(temp_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R1-Zero\n",
    "n = 5\n",
    "model = 'DeepSeek-R1-Zero'\n",
    "for i in range(n):\n",
    "    print(i)\n",
    "    temp_df = gen_responses(dat, model, i)\n",
    "    print(temp_df)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
